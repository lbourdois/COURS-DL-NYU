---
permalink: /
title: "Cours d'apprentissage profond de l'UniversitÃ© de New York"
excerpt: "Le cours de Yann LE CUN et Alfredo CANZIANI<br> traduit en franÃ§ais par LoÃ¯ck BOURDOIS"
header:
   overlay_color: "#57068C"
layout: splash
---

<h1 style="text-align: center;">Avant-propos</h1>
<p> Ce cours porte sur les rÃ©centes techniques de reprÃ©sentation et dâ€™apprentissage profond. Il se concentre sur lâ€™apprentissage supervisÃ©, non supervisÃ© et autosupervistÃ©, mais aussi sur les mÃ©thodes dâ€™enchÃ¢ssement, les rÃ©seaux convolutifs et rÃ©currents.
 Il est illustrÃ© dâ€™applications Ã  la vision par ordinateur, la comprÃ©hension du langage naturel et la reconnaissance vocale.</p> 

<p> Pour suivre ce cours, il est fortement conseillÃ© dâ€™avoir des prÃ©requis en algÃ¨bre et dâ€™avoir dÃ©jÃ  suivi un cours introductif dâ€™apprentissage machine ou de science des donnÃ©es. Ces cours sont destinÃ©s Ã  des personnes de niveau bac+4 ou bac+5.</p> 


<br><br><br>
<h1 style="text-align: center;", color="#57068C">Programme</h1>

<p>
Nous vous invitons Ã  privilÃ©gier les vidÃ©os de la [chaine YouTube](https://www.youtube.com/watch?v=0bMe_vCZo30&list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq) (contenu Â« officiel Â») puisque le cours y est donnÃ© par le corps enseignant contrairement au site web oÃ¹ il sâ€™agit des notes prises par les Ã©tudiants pendant le cours.
Le site web Ã©tant des rÃ©sumÃ©s des vidÃ©os, celles-ci comprennent donc gÃ©nÃ©ralement des informations supplÃ©mentaires par rapport au site. Comme par exemple :<br>
- des anecdotes sur les diffÃ©rents concepts abordÃ©s,<br>
- des blagues, <br>
- la rÃ©pÃ©tition dâ€™un mÃªme concept mais sous la forme de diffÃ©rentes formulations permettant ainsi gÃ©nÃ©ralement de comprendre une idÃ©e si une premiÃ¨re formulation nâ€™est pas saisie,<br> 
- les questions des Ã©tudiants qui peuvent Ãªtre celles que vous ayez vous-mÃªme pendant le visionnage,<br>
Notez que si des concepts ne sont toujours pas compris Ã  lâ€™issue de la vidÃ©o, vous avez la possibilitÃ© de poser une question en commentaire de la vidÃ©o YouTube, ce que ne permet pas le site web.<br>
- les rÃ©fÃ©rences des articles sur lesquels se basent le cours sont prÃ©sentes sur les diapositives des vidÃ©os alors quâ€™elles sont absentes du site. <br><br>
  
Le site web sert ainsi davantage de rÃ©sumÃ© des vidÃ©os ou encore de base que vous pouvez rÃ©utiliser pour vos notes personnelles que vous prenez pendant le visionnage des vidÃ©os. 
En cas de besoin vous pouvez facilement basculer du site Ã  un moment dâ€™une vidÃ©o donnÃ©e en cliquant sur les titres des paragraphes des pages web.
</p>

* Semaine 1
* Titre vidÃ©o CM
* Petite description
* Titre vidÃ©o TD
* Petite description + Slides
* Notebook 


<table>
<!-- =============================== HEADER ================================ -->
  <thead>
    <tr>
      <th>Semaine</th>
      <th align="left">Format</th>
      <th align="left">Titre</th>
      <th align="left">Resources</th>
    </tr>
  </thead>
  <tbody>
<!-- =============================== SEMAINE 1 ================================ -->
    <tr>
      <td rowspan="3" align="center"><a href="{{site.baseurl}}/fr/week01/01">â‘ </a></td>
      <td rowspan="2">Cours magistral</td>
      <td><a href="{{site.baseurl}}/fr/week01/01-1">Histoire et motivations</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1Q7LtZyIS1f3TfeTGll3aDtWygh3GAfCb">ğŸ–¥ï¸</a>
        <a href="https://www.youtube.com/watch?v=0bMe_vCZo30">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="{{site.baseurl}}/fr/week01/01-2">Evolution et Apprentissage profond</a></td></tr>
    <tr>
      <td rowspan="1">Travaux dirigÃ©s</td>
      <td><a href="{{site.baseurl}}/fr/week01/01-3">RÃ©seaux de neurones</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/01-tensor_tutorial.ipynb">ğŸ““</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/02-space_stretching.ipynb">ğŸ““</a>
        <a href="https://www.youtube.com/watch?v=5_qrxVq1kvc">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== SEMAINE 2 ================================ -->
    <tr>
      <td rowspan="3" align="center"><a href="{{site.baseurl}}/fr/week02/02">â‘¡</a></td>
      <td rowspan="2">Cours magistral</td>
      <td><a href="{{site.baseurl}}/fr/week02/02-1">Descente de gradient stochastique et rÃ©tropropagation</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1w2jV_BT2hWzfOKBR02x_rB4-dfVUI6SR">ğŸ–¥ï¸</a>
        <a href="https://www.youtube.com/watch?v=d9vdh3b787Y">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="{{site.baseurl}}/fr/week02/02-2">La rÃ©tropropagation en pratique</a></td></tr>
    <tr>
      <td rowspan="1">Travaux dirigÃ©s</td>
      <td><a href="{{site.baseurl}}/fr/week02/02-3">EntraÃ®nement dâ€™un rÃ©seau de neurones</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/01%20-%20Spiral%20classification.pdf">ğŸ–¥</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/04-spiral_classification.ipynb">ğŸ““</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/05-regression.ipynb">ğŸ““</a>
        <a href="https://www.youtube.com/watch?v=WAn6lip5oWk">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== SEMAINE 3 ================================ -->
    <tr>
      <td rowspan="3" align="center"><a href="{{site.baseurl}}/fr/week03/03">â‘¢</a></td>
      <td rowspan="2">Cours magistral</td>
      <td><a href="{{site.baseurl}}/fr/week03/03-1">Transformation des paramÃ¨tres</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=18UFaOGNKKKO5TYnSxr2b8dryI-PgZQmC">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/FW5gFiJb-ig">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="{{site.baseurl}}/fr/week03/03-2">RÃ©seaux de neurones convolutifs (ConvNets)</a></td></tr>
    <tr>
      <td rowspan="1">Travaux dirigÃ©s</td>
      <td><a href="{{site.baseurl}}/fr/week03/03-3">PropriÃ©tÃ©s des signaux naturels</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/02%20-%20CNN.pdf">ğŸ–¥</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/06-convnet.ipynb">ğŸ““</a>
        <a href="https://youtu.be/kwPWpVverkw">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== SEMAINE 4 ================================ -->
    <tr>
      <td rowspan="1" align="center"><a href="{{site.baseurl}}/fr/week04/04">â‘£</a></td>
      <td rowspan="1">Travaux dirigÃ©s</td>
      <td><a href="{{site.baseurl}}/fr/week04/04-1">Convolution Ã  1 dimension</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/07-listening_to_kernels.ipynb">ğŸ““</a>
        <a href="https://youtu.be/OrBEon3VlQg">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== SEMAINE 5 ================================ -->
    <tr>
      <td rowspan="3" align="center"><a href="{{site.baseurl}}/fr/week05/05">â‘¤</a></td>
      <td rowspan="2">Cours magistral</td>
      <td><a href="{{site.baseurl}}/fr/week05/05-1">Optimisation I</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1pwlGN6hDFfEYQqBqcMjWbe4yfBDTxsab">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/--NZb480zlg">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="{{site.baseurl}}/fr/week05/05-2">Optimisation II</a></td></tr>
    <tr>
      <td rowspan="1">Travaux dirigÃ©s</td>
      <td><a href="{{site.baseurl}}/fr/week05/05-3">ConvNets, autograd</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/03-autograd_tutorial.ipynb">ğŸ““</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/extra/b-custom_grads.ipynb">ğŸ““</a>
        <a href="https://youtu.be/eEzCZnOFU1w">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== SEMAINE 6 ================================ -->
    <tr>
      <td rowspan="3" align="center"><a href="{{site.baseurl}}/fr/week06/06">â‘¥</a></td>
      <td rowspan="2">Cours magistral</td>
      <td><a href="{{site.baseurl}}/fr/week06/06-1">Applications des ConvNets</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1opT7lV0IRYJegtZjuHsKhlsM5L7GpGL1">ğŸ–¥ï¸</a>
        <a href="https://drive.google.com/open?id=1sdeVBC3nuh5Zkm2sqzdScEicRvLc_v-F">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/ycbMGyCPzvE">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="{{site.baseurl}}/fr/week06/06-2">RÃ©seaux de neurones rÃ©currents (RNNs) et Attention</a></td></tr>
    <tr>
      <td rowspan="1">Travaux dirigÃ©s</td>
      <td><a href="{{site.baseurl}}/fr/week06/06-3">EntraÃ®ner des RNNs</a></td>
      <td>
	<a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/04%20-%20RNN.pdf">ğŸ–¥ï¸</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/08-seq_classification.ipynb">ğŸ““</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/09-echo_data.ipynb">ğŸ““</a>
        <a href="https://youtu.be/8cAffg2jaT0">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== SEMAINE 7 ================================ -->
    <tr>
      <td rowspan="3" align="center"><a href="{{site.baseurl}}/fr/week07/07">â‘¦</a></td>
      <td rowspan="2">Cours magistral</td>
      <td><a href="{{site.baseurl}}/fr/week07/07-1">ModÃ¨les Ã  base dâ€™Ã©nergie (EBMs)</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1z8Dz1YtkOEJpU-gh5RIjORs3GGqkYJQa">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/tVwV14YkbYs">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="{{site.baseurl}}/fr/week07/07-2">Apprentissage autosupervisÃ© et EBMs</a></td></tr>
    <tr>
      <td rowspan="1">Travaux dirigÃ©s</td>
      <td><a href="{{site.baseurl}}/fr/week07/07-3">Auto-encodeurs</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/05%20-%20Generative%20models.pdf">ğŸ–¥ï¸</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/10-autoencoder.ipynb">ğŸ““</a>
        <a href="https://youtu.be/bggWQ14DD9M">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== SEMAINE 8 ================================ -->
    <tr>
      <td rowspan="3" align="center"><a href="{{site.baseurl}}/fr/week08/08">â‘§</a></td>
      <td rowspan="2">Cours magistral</td>
      <td><a href="{{site.baseurl}}/fr/week08/08-1">MÃ©thodes contrastives</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1Zo_PyBEO6aNt0GV74kj8MQL7kfHdIHYO">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/ZaVP2SY23nc">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="{{site.baseurl}}/fr/week08/08-2">Variable latente rÃ©gularisÃ©e</a></td></tr>
    <tr>
      <td rowspan="1">Travaux dirigÃ©s</td>
      <td><a href="{{site.baseurl}}/fr/week08/08-3">EntraÃ®ner des Auto-Encodeurs Variationnels (VAEs)</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/05%20-%20Generative%20models.pdf">ğŸ–¥ï¸</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/11-VAE.ipynb">ğŸ““</a>
        <a href="https://youtu.be/7Rb4s9wNOmc">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== SEMAINE 9 ================================ -->
    <tr>
      <td rowspan="3" align="center"><a href="{{site.baseurl}}/fr/week09/09">â‘¨</a></td>
      <td rowspan="2">Cours magistral</td>
      <td><a href="{{site.baseurl}}/fr/week09/09-1">EparsitÃ©</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=1wJRzhjSqlrSqEpX4Omagb_gdIkQ5f-6K">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/Pgct8PKV7iw">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="{{site.baseurl}}/fr/week09/09-2">ModÃ¨les du monde, RÃ©seaux gÃ©nÃ©ratifs antagonistes (GANs)</a></td></tr>
    <tr>
      <td rowspan="1">Travaux dirigÃ©s</td>
      <td><a href="{{site.baseurl}}/fr/week09/09-3">EntraÃ®ner des GANs</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/05%20-%20Generative%20models.pdf">ğŸ–¥ï¸</a>
        <a href="https://github.com/pytorch/examples/tree/master/dcgan">ğŸ““</a>
        <a href="https://youtu.be/xYc11zyZ26M">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== SEMAINE 10 =============================== -->
    <tr>
      <td rowspan="3" align="center"><a href="{{site.baseurl}}/fr/week10/10">â‘©</a></td>
      <td rowspan="2">Cours magistral</td>
      <td><a href="{{site.baseurl}}/fr/week10/10-1">Apprentissage autosupervisÃ© appliquÃ© Ã  la vision par ordinateur I</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/open?id=16lsnDN2HIBTcRucbVKY5B_U16c0tNQhR">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/0KeR6i1_56g">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="{{site.baseurl}}/fr/week10/10-2"> Apprentissage autosupervisÃ© appliquÃ© Ã  la vision par ordinateur II</a></td></tr>
    <tr>
      <td rowspan="1">Travaux dirigÃ©s</td>
      <td><a href="{{site.baseurl}}/fr/week10/10-3">ContrÃ´le prÃ©dictif</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/09%20-%20Controller%20learning.pdf">ğŸ–¥ï¸</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/14-truck_backer-upper.ipynb">ğŸ““</a>
        <a href="https://youtu.be/A3klBqEWR-I">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== SEMAINE 11 =============================== -->
    <tr>
      <td rowspan="3" align="center"><a href="{{site.baseurl}}/fr/week11/11">â‘ª</a></td>
      <td rowspan="2">Cours magistral</td>
      <td><a href="{{site.baseurl}}/fr/week11/11-1">Fonctions dâ€™activation</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/file/d/1AzFVLG7D4NK6ugh60f0cJQGYF5OL2sUB">ğŸ–¥ï¸</a>
        <a href="https://drive.google.com/file/d/1rkiZy0vjZqE2w7baVWvxwfAGae0Eh1Wm">ğŸ–¥ï¸</a>
        <a href="https://drive.google.com/file/d/1tryOlVAFmazLLZusD2-UfReFMkPk5hPk">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/bj1fh3BvqSU">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="{{site.baseurl}}/fr/week11/11-2">Fonctions de perte</a></td></tr>
    <tr>
      <td rowspan="1">Travaux dirigÃ©s</td>
      <td><a href="{{site.baseurl}}/fr/week11/11-3">Prediction et apprentissage d'une politique sous incertitude</a></td>
      <td>
        <a href="http://bit.ly/PPUU-slides">ğŸ–¥ï¸</a>
        <a href="http://bit.ly/PPUU-code">ğŸ““</a>
        <a href="https://youtu.be/VcrCr-KNBHc">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== SEMAINE 12 =============================== -->
    <tr>
      <td rowspan="3" align="center"><a href="{{site.baseurl}}/fr/week12/12">â‘«</a></td>
      <td rowspan="2">Cours magistral</td>
      <td><a href="{{site.baseurl}}/fr/week12/12-1">Apprentissage profond pour le traitement du langage naturel I</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/file/d/149m3wRavTp4DQZ6RJTej8KP8gv4jnkPW/">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/6D4EWKJgNn0">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="{{site.baseurl}}/fr/week12/12-2"> Apprentissage profond pour le traitement du langage naturel II</a></td></tr>
    <tr>
      <td rowspan="1">Travaux dirigÃ©s</td>
      <td><a href="{{site.baseurl}}/fr/week12/12-3">Attention & Transformer</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/10%20-%20Attention%20%26%20transformer.pdf">ğŸ–¥ï¸</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/15-transformer.ipynb">ğŸ““</a>
        <a href="https://youtu.be/f01J0Dri-6k">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== SEMAINE 13 =============================== -->
    <tr>
      <td rowspan="3" align="center"><a href="{{site.baseurl}}/fr/week13/13">â‘¬</a></td>
      <td rowspan="2">Cours magistral</td>
      <td><a href="{{site.baseurl}}/fr/week13/13-1"> RÃ©seau convolutif pour graphe I</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/file/d/1oq-nZE2bEiQjqBlmk5_N_rFC8LQY0jQr/">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/Iiv9R6BjxHM">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="{{site.baseurl}}/fr/week13/13-2"> RÃ©seau convolutif pour graphe II</a></td></tr>
    <tr>
      <td rowspan="1">Travaux dirigÃ©s</td>
      <td><a href="{{site.baseurl}}/fr/week13/13-3"> RÃ©seau convolutif pour graphe III</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/11%20-%20GCN.pdf">ğŸ–¥ï¸</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/16-gated_GCN.ipynb">ğŸ““</a>
        <a href="https://youtu.be/2aKXWqkbpWg">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== SEMAINE 14 =============================== -->
    <tr>
      <td rowspan="3" align="center"><a href="{{site.baseurl}}/fr/week14/14">â‘­</a></td>
      <td rowspan="2">Cours magistral</td>
      <td><a href="{{site.baseurl}}/fr/week14/14-1">PrÃ©diction utilisant la structure</a></td>
      <td rowspan="2">
        <a href="https://drive.google.com/file/d/1qBu-2hYWaGYEXeX7kAU8O4S2RZ1hMjsk/">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/gYayCG6YyO8">ğŸ¥</a>
      </td>
    </tr>
    <tr><td><a href="{{site.baseurl}}/fr/week14/14-2">MÃ©thodes graphiques</a></td></tr>
    <tr>
      <td rowspan="1">Travaux dirigÃ©s</td>
      <td><a href="{{site.baseurl}}/fr/week14/14-3">RÃ©gularisation et rÃ©seaux bayÃ©siens</a></td>
      <td>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/07%20-%20Regularisation.pdf">ğŸ–¥ï¸</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/12-regularization.ipynb">ğŸ““</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/08%20-%20Bayesian%20NN.pdf">ğŸ–¥ï¸</a>
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/13-bayesian_nn.ipynb">ğŸ““</a>
        <a href="https://youtu.be/DL7iew823c0">ğŸ¥</a>
      </td>
    </tr>
<!-- =============================== SEMAINE 15 =============================== -->
    <tr>
      <td rowspan="2" align="center"><a href="{{site.baseurl}}/fr/week15/15">â‘®</a></td>
      <td rowspan="2">Travaux dirigÃ©s</td>
      <td><a href="{{site.baseurl}}/fr/week15/15-1">InfÃ©rence pour les EBMs Ã  variable latente</a></td>
      <td rowspan="1">
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/12%20-%20EBM.pdf">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/sbhr2wjU1-I">ğŸ¥</a>
      </td>
    </tr>
    <tr>
      <td><a href="{{site.baseurl}}/fr/week15/15-2">EntraÃ®nement des EBMs Ã  variable latente</a></td>
      <td rowspan="1">
        <a href="https://github.com/Atcold/pytorch-Deep-Learning/blob/master/slides/12%20-%20EBM.pdf">ğŸ–¥ï¸</a>
        <a href="https://youtu.be/XLSb1Cs1Jao">ğŸ¥</a>
      </td>
    </tr>
  </tbody>
</table>


<br><br>
<h1 style="text-align: center;">Personnes</h1>
<div class="feature__wrapper" style="text-align: center;">

    <div class="feature__item2">
        <div class="archive__item">
            <div class="archive__item-teaser">
                <img src="{{site.baseurl}}/assets/images/Yann.jpg" style="margin: 0 auto;border: 7px solid #eeeeee;border-radius: 50%;">
            </div>

            <div class="archive__item-body">
                <h3 class="archive__item-title">Yann LE CUN</h3>

                <div class="archive__item-excerpt">
                    Description Ã  ajouter<br>
		<a href="yann@cs.nyu.edu">Email</a> | <a href="https://twitter.com/ylecun">Twitter</a> | <a href="https://www.linkedin.com/in/yann-lecun/">LinkedIn</a> | <a href="http://yann.lecun.com/">Site web</a>
                </div>

            </div>
        </div>
    </div>
    <div class="feature__item2">
        <div class="archive__item">
            <div class="archive__item-teaser">
                <img src="{{site.baseurl}}/assets/images/Alfredo.jpg" style="margin: 0 auto;border: 7px solid #eeeeee;border-radius: 50%;">
            </div>

            <div class="archive__item-body">
                <h3 class="archive__item-title">Alfredo CANZIANI</h3>

                <div class="archive__item-excerpt">
                    Description Ã  ajouter<br>
		<a href="canziani@nyu.edu">Email</a> | <a href="https://twitter.com/alfcnz">Twitter</a> | <a href="https://www.linkedin.com/in/alfredocanziani">LinkedIn</a> | <a href="https://atcold.github.io/">Site web</a>
                </div>

            </div>
        </div>
    </div>
	
	    <div class="feature__item2">
        <div class="archive__item">
            <div class="archive__item-teaser">
                <img src="{{site.baseurl}}/assets/images/LoÃ¯ck.jpg" style="margin: 0 auto;border: 7px solid #eeeeee;border-radius: 50%;">
            </div>

            <div class="archive__item-body">
                <h3 class="archive__item-title">LoÃ¯ck BOURDOIS</h3>

                <div class="archive__item-excerpt">
                    Description Ã  ajouter<br>
		<a href="loick.bourdois@outlook.com">Email</a> | <a href="https://twitter.com/BdsLoick">Twitter</a> | <a href="https://www.linkedin.com/in/lo%C3%AFck-bourdois-111488171/">LinkedIn</a> | <a href="https://lbourdois.github.io/blog/">Site web</a>
                </div>

            </div>
        </div>
    </div>
</div>

<br><br><br>
<h1 style="text-align: center;">FAQ</h1>

Voici quelques rÃ©ponses Ã  des questions frÃ©quemment posÃ©es :<br><br>

- <b>Est-ce que suivre ce cours permet dâ€™obtenir une certification ?</b><br>
<cite> Non. Pour proposer une certification, il faudrait pouvoir vous Ã©valuer or le contenu nâ€™a pas Ã©tÃ© prÃ©vu pour (contrairement Ã  un MOOC par exemple).   
Cette demande Ã©tant frÃ©quente, des rÃ©flexions sont menÃ©es pour essayer dâ€™en proposer une pour des Ã©ditions (anglaises) futures du cours.</cite><br>
<br>
- <b>Combien de temps consacrer Ã  ce cours ?</b><br>
<cite> Pour chaque semaine, il y a environ 2h30/3h de contenu vidÃ©o. Avec le temps consacrÃ© Ã  la prise de notes et celui pour jouer avec les <i>notebooks</i>, une estimation totale de 5h par semaine semble raisonnable. Pour la suite, cela dÃ©pend du niveau d'immersion que vous voulez atteindre dans un sujet donnÃ© (lire les articles donnÃ©s en rÃ©fÃ©rence, appliquer ce qui a Ã©tÃ© vu en classe Ã  vos propres projets, etc.).</cite><br>
<br>
- <b>OÃ¹ poser une question Ã  lâ€™issue du visionnage dâ€™une vidÃ©o ?</b><br>
<cite> Vous pouvez la poser directement (en anglais) dans la section commentaires sous la vidÃ©o YouTube en question, Alfredo se fera un plaisir dâ€™y rÃ©pondre. Si cette question porte sur un point prÃ©cis de la vidÃ©o, pensez Ã  indiquer lâ€™horodatage. Vous pouvez le faire Ã©galement sur le <a href="https://discord.gg/CthuqsX8Pb">Discord</a> de la classe dÃ©diÃ© expressÃ©ment aux Ã©tudiants. Il sert Ã©galement Ã  coordonner des groupes de visionnage, discuter des devoirs, suggÃ©rer des amÃ©liorations ou plus gÃ©nÃ©ralement pour tout sujet liÃ© au cours.</cite><br>
<br>
- <b>Puis-je utiliser ce cours ?</b><br>
<cite> Bien sÃ»r, le cours est placÃ© sous la <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.fr"><i>Licence internationale Creative Commons Attribution-NonCommercial-ShareAlike 4.0</i></a>.
Cela signifie que :<br>
- Vous n'Ãªtes pas autorisÃ© Ã  faire un usage commercial de cette Å“uvre.<br>
- Vous devez crÃ©diter l'Å“uvre, intÃ©grer un lien vers la licence et indiquer si des modifications ont Ã©tÃ© effectuÃ©es Ã  l'Å“uvre. Vous devez indiquer ces informations par tous les moyens raisonnables, sans toutefois suggÃ©rer que l'offrant vous soutient ou soutient la faÃ§on dont vous avez utilisÃ© son Å“uvre.<br>
- Dans le cas oÃ¹ vous effectuez un remix, que vous transformez, ou crÃ©ez Ã  partir du matÃ©riel Ã  partir de l'Å“uvre originale, vous devez diffuser l'Å“uvre modifiÃ©e dans les mÃªmes conditions, c'est Ã  dire avec la mÃªme licence avec laquelle l'Å“uvre originale a Ã©tÃ© diffusÃ©e.<br>   
- Pour le crÃ©dit, vous pouvez utiliser le BibTeX suivant :</cite><br>
<code>
@misc{canziani2020nyudlsp20,<br>
  author = {Canziani, Alfredo and LeCun, Yann and Bourdois, LoÃ¯ck},<br>
  title = {Cours dâ€™apprentissage profond de lâ€™UniversitÃ© de New York},<br>
  howpublished = "\url{https://lbourdois.github.io/cours-dl-nyu/}",<br>
  year = {2023}"}</code>
  
  
<br><br><br><br>
<h1 style="text-align: center;">Choix de traduction</h1>

- Choix de traduire les termes anglais en franÃ§ais :<br><br>
	
<table>
<!-- =============================== HEADER ================================ -->
  <thead>
    <tr>
      <th>Terme</th>
      <th>Traduction</th>
      <th>Raisons / Explications</th>
    </tr>
  </thead>
  <tbody>
<!-- =============================== Ligne 1 ================================ -->
    <tr>
      <td rowspan="1">Chain rule</td>
      <td rowspan="1">RÃ¨gle de dÃ©rivation des fonctions composÃ©es</td>
      <td>En pratique usage du terme Â« rÃ¨gle de la chaÃ®ne Â» dans les sous-titres des vidÃ©os pour gagner de la place.</td>
    </tr>
<!-- =============================== Ligne 2 ================================ -->
    <tr>
      <td rowspan="1">CNN</td>
      <td rowspan="1">ConvNet</td>
      <td>Yann tient particuliÃ¨rement au respect de cette traduction. Voir notamment la page 202 du livre <a href="https://www.odilejacob.fr/catalogue/sciences/informatique/quand-la-machine-apprend_9782738149312.php">Quand la machine apprend</a>.</td>
    </tr>
<!-- =============================== Ligne 3 ================================ -->
    <tr>
      <td rowspan="1">Downstream tasks</td>
      <td rowspan="1">TÃ¢ches en aval</td>
      <td>Les tÃ¢ches de prÃ©texte Ã©tant les tÃ¢ches en amont.</td>
    </tr>
<!-- =============================== Ligne 4 ================================ -->
    <tr>
      <td rowspan="1">Energy-Based Models</td>
      <td rowspan="1">ModÃ¨les Ã  base dâ€™Ã©nergie</td>
      <td>Traduction pas forcÃ©ment satisfaisante mais adoptÃ©e faute de mieux.</td>
    </tr>
<!-- =============================== Ligne 5 ================================ -->
    <tr>
      <td rowspan="1">Embedding</td>
      <td rowspan="1">EnchÃ¢ssement</td>
      <td>Reprise de la traduction utilisÃ©e page 228 dans le livre <i>Quand la machine apprend</i>. Dans la littÃ©rature, il est possible de trouver Ã©galement l'usage du terme Â« plongement Â» comme traduction. Parler tout simplement de vectorisation paraÃ®trait beaucoup plus simple pour faire le lien avec le concept mathÃ©matique (on vectorise un mot par exemple).</td>
    </tr>
<!-- =============================== Ligne 6 ================================ -->
    <tr>
      <td rowspan="1">Graph Neural Networks</td>
      <td rowspan="1">RÃ©seaux de neurones pour graphe</td>
      <td>En pratique, pour les sous-titres des vidÃ©os, l'abrÃ©viation GNN est privilÃ©giÃ©e.</td>
    </tr>
<!-- =============================== Ligne 7 ================================ -->
    <tr>
      <td rowspan="1">Graph Convolution Networks</td>
      <td rowspan="1">RÃ©seaux convolutifs pour graphe</td>
      <td>En pratique, pour les sous-titres des vidÃ©os, l'abrÃ©viation GCN est privilÃ©giÃ©e.</td>
    </tr>
<!-- =============================== Ligne 8 ================================ -->
    <tr>
      <td rowspan="1">Manifold</td>
      <td rowspan="1">VariÃ©tÃ©</td>
      <td>Voir <a href="https://fr.wikipedia.org/wiki/Vari%C3%A9t%C3%A9_(g%C3%A9om%C3%A9trie)">l'article WikipÃ©dia</a></td>
    </tr>
<!-- =============================== Ligne 9 ================================ -->
    <tr>
      <td rowspan="1">Nonlinearity function</td>
      <td rowspan="1">Fonction non linÃ©aire</td>
      <td>En franÃ§ais, on utilise Ã©galement le terme de Â« fonction dâ€™activation Â».</td>
    </tr>
<!-- =============================== Ligne 10 ================================ -->
    <tr>
      <td rowspan="1">Overfitting</td>
      <td rowspan="1">SurentraÃ®nement</td>
      <td>Reprise de la traduction utilisÃ©e page 155 dans le livre <i>Quand la machine apprend</i>.</td>
    </tr>
<!-- =============================== Ligne 11 ================================ -->
    <tr>
      <td rowspan="1">Regularizer</td>
      <td rowspan="1">RÃ©gulariseur</td>
      <td>NÃ©ologisme prÃ©fÃ©rable Ã  rÃ©gularisateur.</td>
    </tr>
<!-- =============================== Ligne 12 ================================ -->
    <tr>
      <td rowspan="1">Sparse</td>
      <td rowspan="1">Epars</td>
      <td>Pour l'expression Â« <i>sparse matrix</i> Â», nous traduisons Â« sparse Â» en Â« creuse Â» pour Â« matrice creuse Â». Pour tous les autres cas nous utilisons Â« Ã©pars Â» ou Â« Ã©parse Â» en fonction du genre du mot auquel l'adjectif se rapporte.</td>
    </tr>
<!-- =============================== Ligne 13 ================================ -->
    <tr>
      <td rowspan="1">Sparsity</td>
      <td rowspan="1">EparsitÃ©</td>
      <td>NÃ©ologisme basÃ© sur le mot Â« Ã©pars Â».</td>
    </tr>
<!-- =============================== Ligne 14 ================================ -->
    <tr>
      <td rowspan="1">Template Matching</td>
      <td rowspan="1">Template Matching</td>
      <td>L'expression Â« appariement de patrons Â» comme traduction peut Ãªtre trouvable sur le site ou dans les vidÃ©os.</td>
    </tr>	
<!-- =============================== Ligne 15 ================================ -->
    <tr>
      <td rowspan="1">Yann LeCun</td>
      <td rowspan="1">Yann Le Cun ou Yann</td>
      <td> L'explication de l'Ã©criture du nom de famille est donnÃ©e page 193 du livre <i>Quand la machine apprend</i>. Dans les notes en anglais des Ã©tudiants, il est possible de trouver Â« <i>Mr Yann LeCun</i> Â», Â« <i>Mr LeCun</i> Â», Â« <i>Doctor Yann LeCun</i> Â», Â« <i>Professor LeCun</i> Â», etc. Nous utilisons simplement Â« Yann Â».</td>
    </tr>
  </tbody>
</table>

<br> 
 - Choix de ne pas traduire les termes anglais en franÃ§ais :<br> 
Nous avons fait le choix de ne pas traduire certains termes anglais pour des raisons pratiques. Par exemple, certains concepts nÃ©cessitent 3 ou 4 mots en franÃ§ais lÃ  oÃ¹ 1 seul suffit en anglais. Cela pose notamment problÃ¨me pour les vidÃ©os oÃ¹ le temps d'affichage est limitÃ©, d'oÃ¹ la prÃ©fÃ©rence Ã  garder le terme en anglais. Il serait possible d'utiliser des nÃ©ologismes mais nous avons prÃ©fÃ©rÃ© ne pas en imposer car ne pouvant peut-Ãªtre pas faire consensus. Sur le site, les mots laissÃ©s en anglais sont indiquÃ©s en italique. 
<br><br>
	
<table>
<!-- =============================== HEADER ================================ -->
  <thead>
    <tr>
      <th>Terme</th>
      <th>Traduction</th>
      <th>Raisons / Explications</th>
    </tr>
  </thead>
  <tbody>
<!-- =============================== Ligne 1 ================================ -->
    <tr>
      <td rowspan="1">Dropout</td>
      <td rowspan="1">Dropout</td>
      <td>Le mot Â« dÃ©cimation Â» serait appropriÃ© mais il est dÃ©jÃ  utilisÃ© en traitement du signal pour signifier Â« sous-Ã©chantillonnage Â».</td>
    </tr>
<!-- =============================== Ligne 2 ================================ -->
    <tr>
      <td rowspan="1">Finetuning</td>
      <td rowspan="1">Finetuning</td>
      <td>Le terme Â« affinage Â» peut Ãªtre trouvable dans la littÃ©rature. </td>
    </tr>
<!-- =============================== Ligne 3 ================================ -->
    <tr>
      <td rowspan="1">One hot</td>
      <td rowspan="1">One hot</td>
      <td>La notion de Â« vecteurs de base canonique Â» pourrait Ãªtre utilisÃ©e mais elle est un peu technique et l'expression est plutÃ´t longue pour traduire Ã  peine 2 mots.  N.D.T : lorsque j'Ã©tais Ã©tudiant, dans mes cours d'algÃ¨bre linÃ©aire, j'utilisais soit Â« v.b.c Â» pour Â« vecteurs de base canonique Â» ou bien Â« zÃ©run Â» (pour un vecteur contenant des 0 et un 1) mais il s'agit d'une convention personnelle que je ne prÃ©fÃ¨re pas imposer.</td>
    </tr>
<!-- =============================== Ligne 4 ================================ -->
    <tr>
      <td rowspan="1">Pooling</td>
      <td rowspan="1">Pooling</td>
      <td>Plusieurs traductions envisagÃ©es comme agrÃ©gation, agglomÃ©ration, ou coalescence. Garder le terme en anglais est plus simple (un Â« max-agrÃ©gation Â» n'est pas trÃ¨s Ã©lÃ©gant par exemple).</td>
    </tr>
<!-- =============================== Ligne 5 ================================ -->
    <tr>
      <td rowspan="1">Transformer</td>
      <td rowspan="1">Transformer</td>
      <td>A COMPLETER.</td>
    </tr>
  </tbody>
</table>

<br><br><br>
<h1 style="text-align: center;">Jeu de donnÃ©es</h1>
La traduction du cours de l'anglais vers le franÃ§ais a permis de crÃ©er un corpus de donnÃ©es parallÃ¨les. Celui-ci est tÃ©lÃ©chargeable via le lien suivant ou sur le Hub d'Hugging Face.

	  
<br><br><br><br>
<h1 style="text-align: center;">Remerciements</h1>
Un grand merci aux confÃ©renciers invitÃ©s qui sont venus donner des prÃ©sentations dans le cadre du cours :<br>
<a href="https://twitter.com/aaron_defazio">Aaron Defazio</a>, <a href="https://twitter.com/awnihannun">Awni Hannun</a>, <a href="https://twitter.com/imisra_">Ishan Misra</a>, <a href="https://twitter.com/MarcRanzato">Marc'Aurelio Ranzato</a>, <a href="https://twitter.com/ml_perception">Mike Lewis</a> et <a href="https://twitter.com/xbresson">Xavier Bresson</a>.<br>
<br>
Un autre grand merci aux personnes de l'ombre : <a href="https://twitter.com/marikgoldstein">Mark Goldstein</a>, <a href="https://twitter.com/ebetica">Zeming Lin</a> et <a href="https://twitter.com/jiachenai">Jiachen Zhu</a>.<br>

<br>
Enfin un Ã©norme merci au plus de 190 Ã©tudiants qui ont partagÃ© leur notes de cours :<br>
Yunya Wang, SunJoo Park, Mark Estudillo, Justin Mae,
Marina Zavalina, Peeyush Jain, Adrian Pearl, Davida Kollmar,
Derek Yen, Tony Xu, Ben Stadnick, Prasanthi Gurumurthy,
Amartya Prasad, Dongning Fang, Yuxin Tang, Sahana Upadhya,
Micaela Flores, Sheetal Laad, Brina Seidel, Aishwarya Rajan,
Jiuhong Xiao, Trieu Trinh, Elliot Silva, Calliea Pan,
Chris Ick, Soham Tamba, Ziyu Lei, Hengyu Tang,
Ashwin Bhola, Nyutian Long, Linfeng Zhang, Poornima Haridas,
Yuchi Ge, Anshan He, Shuting Gu, Weiyang Wen,
Vaibhav Gupta, Himani Shah, Gowri Addepalli, Lakshmi Addepalli,
Guido Petri, Haoyue Ping, Chinmay Singhal, Divya Juneja,
Leyi Zhu, Siqi Wang, Tao Wang, Anqi Zhang,
Shiqing Li, Chenqin Yang, Yakun Wang, Jimin Tan,
Jiayao Liu, Jialing Xu, Zhengyang Bian, Christina Dominguez,
Zhengyuan Ding, Biao Huang, Lin Jiang, Nhung Le,
Karanbir Singh Chahalï¼ŒMeiyi He, Alexander Gao, Weicheng Zhu,
Ravi Choudharyï¼ŒB V Nithish Addepalli, Syed Rahmanï¼ŒJiayi Du,
Xinmeng Li, Atul Gandhi, Li Jiang, Xiao Li,
Vishwaesh Rajiv, Wenjun Qu, Xulai Jiang, Shuya Zhao,
Henry Steinitz, Rutvi Malaviya, Aathira Manoj,
Richard Pang, Aja Klevs, Hsin-Rung Chou, Mrinal Jain,
Kelly Sooch, Anthony Tse, Arushi Himatsingka, Eric Kosgey,
Bofei Zhang, Andrew Hopen, Maxwell Goldstein, Zeping Zhan,
William Huang, Kunal Gadkar, Gaomin Wu, Lin Ye,
Aniket Bhatnagar, Dhruv Goyal, Cole Smith, Nikhil Supekar,
Zhonghui Hu, Yuqing Wang, Alfred Ajay Aureate Rajakumar, Param Shah,
Muyang Jin, Jianzhi Li, Jing Qian, Zeming Lin,
Haochen Wang, Eunkyung An, Ying Jin, Ningyuan Huang,
Charles Brillo-Sonnino, Shizhan Gong, Natalie Frank, Yunan Hu,
Anuj Menta, Dipika Rajesh, Vikas Patidar, Mohith Damarapati,
Jiayu Qiu, Yuhong Zhu, Lyuang Fu, Ian Leefmans,
Trevor Mitchell, Andrii Dobroshynskyi, Shreyas Chandrakaladharan, Ben Wolfson,
Francesca Guiso, Annika Brundyn, Noah Kasmanoff, Luke Martin,
Bilal Munawar, Alexander Bienstock, Can Cui, Shaoling Chen,
Neil Menghani, Tejaishwarya Gagadam, Joshua Meisel, Jatin Khilnani,
Go Inoue, Muhammad Osama Khan, Muhammad Shujaat Mirza, Muhammad Muneeb Afzal,
Junrong Zha, Muge Chen, Rishabh Yadav, Zhuocheng Xu,
Yada Pruksachatkun, Ananya Harsh Jha, Joseph Morag, Dan Jefferys-White, Brian Kelly,
Karl Otness, Xiaoyi Zhang, Shreyas Chandrakaladharan, Chady Raach,
Yilang Hao, Binfeng Xu, Ebrahim Rasromani, Mars Wei-Lun Huang,
Anu-Ujin Gerelt-Od, Sunidhi Gupta, Bichen Kou, Binfeng Xu,
Rajashekar Vasantha,
Wenhao Li,
Vidit Bhargava, Monika Dagar,
Nandhitha Raghuram, Xinyi Zhao,
Vasudev Awatramani, Sumit Mamtani,
Srishti Bhargava, Jude Naveen Raj Ilango,
Duc Anh Phi, Krishna Karthik Reddy Jonnala,
Rahul Ahuja, jingshuai jiang,
Cal Peyser, Kevin Chang,
Gyanesh Gupta, Abed Qaddoumi,
Fanzeng Xia, Rohith Mukku,
Angela Teng, Joanna Jin,
Yang Zhou, Daniel Yao
et Sai Charitha Akula.
